{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/opethaiwoh/MovieReview/blob/main/PySpark_TextMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHejWTCXjNsW"
      },
      "source": [
        "\n",
        "**Text Mining Analysis of Movie Dataset**<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuSB8cD1PVIM"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfMBEF8gPV2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40dd99d-e46f-4234-8c5b-55a709e7ba01"
      },
      "source": [
        "#Always check for the updates on: https://apache.osuosl.org/spark\n",
        "!wget -q https://apache.osuosl.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop2.7.tgz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYaovCBcPWEp"
      },
      "source": [
        "#also change the version of the spark to e.g 3.2.4\n",
        "!tar xf spark-3.2.4-bin-hadoop2.7.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q24T1bUPWKQ"
      },
      "source": [
        "pip install -q findspark"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aUoP8F6PWNJ"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#change the spark version here too\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop2.7\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6IYu6zDqPWPp",
        "outputId": "ec455c85-1b6f-4359-b5bb-f00e30794339"
      },
      "source": [
        "os.environ[\"SPARK_HOME\"]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.2.4-bin-hadoop2.7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You don't need to install the latest version"
      ],
      "metadata": {
        "id": "MSzlO0GM6Z5a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5u6GLDhPWSA",
        "outputId": "379a433d-ad41-460d-fb68-1ad0eafc5371"
      },
      "source": [
        "pip install spark-nlp==2.4.2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spark-nlp==2.4.2 in /usr/local/lib/python3.10/dist-packages (2.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the version of spark"
      ],
      "metadata": {
        "id": "X1kLCf1O6P3B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE76oEEBPWUZ",
        "outputId": "e65e6313-f775-4dea-e976-5f8a244a3384"
      },
      "source": [
        "pip install pyspark==3.2.4"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.2.4 in /usr/local/lib/python3.10/dist-packages (3.2.4)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.2.4) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHfTWUKVPWWx",
        "outputId": "6d17a10d-c660-411d-da12-fbd6efb46846"
      },
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version:  2.4.2\n",
            "Apache Spark version:  3.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSkN9p29zT3W",
        "outputId": "c24d7054-6675-4705-fbc2-3d2cccdc9f3e"
      },
      "source": [
        "pip install langid"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langid in /usr/local/lib/python3.10/dist-packages (1.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from langid) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbAAKg9p1diL",
        "outputId": "6bfdcef2-28d4-4c83-a388-87bc7f7318c7"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "VI_pZiu28lhq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('popular')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcYbAGzTqLcG",
        "outputId": "ac1f6e3b-2491-4fd2-bef7-e2e9749a9220"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AzxF--9SJ9p"
      },
      "source": [
        "import string"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mTpk822ecCD"
      },
      "source": [
        "# 14.2. Text Preprocessing\n",
        "# NOTE: Rerun from here if you get SparkContexts error.\n",
        "def check_blanks(data_str):\n",
        "    is_blank = str(data_str.isspace())\n",
        "    return is_blank"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Ix6RkNV3vj"
      },
      "source": [
        "# Remove features\n",
        "import re\n",
        "import string\n",
        "def remove_features(data_str):\n",
        "    # compile regex\n",
        "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
        "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    punc_re = re.compile('[^A-Za-z0-9]+')\n",
        "    num_re = re.compile('(\\\\d+)')\n",
        "    mention_re = re.compile('@(\\w+)')\n",
        "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
        "    # convert to lowercase\n",
        "    data_str = data_str.lower()\n",
        "    # remove hyperlinks\n",
        "    data_str = url_re.sub(' ', data_str)\n",
        "    # remove @mentions\n",
        "    data_str = mention_re.sub(' ', data_str)\n",
        "    # remove puncuation\n",
        "    data_str = punc_re.sub(' ', data_str)\n",
        "    # remove numeric 'words'\n",
        "    data_str = num_re.sub(' ', data_str)\n",
        "    # remove non a-z 0-9 characters and words shorter than 3 characters\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    for word in data_str.split():\n",
        "        if list_pos == 0:\n",
        "            if alpha_num_re.match(word) and len(word) > 2:\n",
        "                cleaned_str = word\n",
        "            else:\n",
        "                cleaned_str = ' '\n",
        "        else:\n",
        "            if alpha_num_re.match(word) and len(word) > 2:\n",
        "                cleaned_str = cleaned_str + ' ' + word\n",
        "            else:\n",
        "                cleaned_str += ' '\n",
        "        list_pos += 1\n",
        "    return cleaned_str"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qkrr-83iC4c",
        "outputId": "d25c4f20-f4b9-411b-c53e-225b16f55bac"
      },
      "source": [
        "# Removes stop words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords.words('english')\n",
        "print(stopwords.words('english'))\n",
        "#print(stops)\n",
        "def remove_stops(data_str):\n",
        "    # expects a string\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    text = data_str.split()\n",
        "    for word in text:\n",
        "        if word not in stopwords:\n",
        "            # rebuild cleaned_str\n",
        "            if list_pos == 0:\n",
        "                cleaned_str = word\n",
        "            else:\n",
        "                cleaned_str = cleaned_str + ' ' + word\n",
        "            list_pos += 1\n",
        "    return cleaned_str"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k6EX89hiGS7",
        "outputId": "7fed32cd-d026-408e-f35c-acb8c4220a95"
      },
      "source": [
        "# Tagging text\n",
        "import nltk\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def tag_and_remove(data_str):\n",
        "    cleaned_str = ' '\n",
        "    # noun tags\n",
        "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n",
        "    # adjectives\n",
        "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
        "    # verbs\n",
        "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
        "\n",
        "    # break string into 'words'\n",
        "    text = data_str.split()\n",
        "\n",
        "    # tag the text and keep only those with the right tags\n",
        "    tagged_text = nltk.pos_tag(text)\n",
        "    for tagged_word in tagged_text:\n",
        "        if tagged_word[1] in nltk_tags:\n",
        "            cleaned_str += tagged_word[0] + ' '\n",
        "\n",
        "    return cleaned_str"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoxXnDTMiILq",
        "outputId": "4a690621-8024-46e9-a900-bf3c0ca9963d"
      },
      "source": [
        "# Lemmatization\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "def lemmatize(data_str):\n",
        "    # expects a string\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    text = data_str.split()\n",
        "    tagged_words = nltk.pos_tag(text)\n",
        "    for word in tagged_words:\n",
        "        if 'v' in word[1].lower():\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='v')\n",
        "        else:\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='n')\n",
        "        if list_pos == 0:\n",
        "            cleaned_str = lemma\n",
        "        else:\n",
        "            cleaned_str = cleaned_str + ' ' + lemma\n",
        "        list_pos += 1\n",
        "    return cleaned_str"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vh_LX_vlvfy",
        "outputId": "040fc5ca-2558-4ca1-8b7c-8e045af8bba7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"MovieRv\").getOrCreate()\n",
        "\n",
        "# Define your UDFs here\n",
        "remove_stops_udf = udf(remove_stops, StringType())\n",
        "remove_features_udf = udf(remove_features, StringType())\n",
        "tag_and_remove_udf = udf(tag_and_remove, StringType())\n",
        "lemmatize_udf = udf(lemmatize, StringType())\n",
        "check_blanks_udf = udf(check_blanks, StringType())\n",
        "\n",
        "# Load your data using SparkSession\n",
        "neg_rdd = spark.sparkContext.textFile(\"/content/drive/Shareddrives/Black in AI/Dataset/combine_neg.txt\")\n",
        "pos_rdd = spark.sparkContext.textFile(\"/content/drive/Shareddrives/Black in AI/Dataset/combine_pos.txt\")\n",
        "neg_parts_rdd = neg_rdd.map(lambda l: l.split(\"\\t\"))\n",
        "pos_parts_rdd = neg_rdd.map(lambda l: l.split(\"\\t\"))\n",
        "neg_filled_rdd = neg_parts_rdd.filter(bool)\n",
        "pos_filled_rdd = pos_parts_rdd.filter(bool)\n",
        "neg_data_df = spark.createDataFrame(neg_filled_rdd, StringType())\n",
        "pos_data_df = spark.createDataFrame(pos_filled_rdd, StringType())\n",
        "\n",
        "# Print the schema of DataFrames\n",
        "neg_data_df.printSchema()\n",
        "pos_data_df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaH5NFiAM5Zx",
        "outputId": "23b8ad64-9f17-4430-af9d-60c839a4f208"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDwGYYNqz0bL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d901b9d2-9b18-4a6e-8127-c5da2b830ce0"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Download NLTK stopwords (only once)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define your UDFs here (replace this with your actual UDF definitions)\n",
        "def remove_stops(text):\n",
        "    # Define your logic to remove stopwords\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stopwords_list]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "remove_stops_udf = udf(remove_stops, StringType())\n",
        "\n",
        "# Define the columns to select from the DataFrame\n",
        "neg_raw_cols = [\"value\"]\n",
        "\n",
        "# Apply the UDF to remove stopwords\n",
        "neg_rm_stops_df = neg_data_df.select(neg_raw_cols).withColumn(\"neg_stop_text\", remove_stops_udf(neg_data_df[\"value\"]))\n",
        "neg_rm_stops_df.show(20)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|               value|       neg_stop_text|\n",
            "+--------------------+--------------------+\n",
            "|[I can only say t...|[I say this: ee03...|\n",
            "|[.maybe.Though I ...|[.maybe.Though I ...|\n",
            "|[..well it doesn'...|[..well really on...|\n",
            "|[and I haven't ev...|[and I even got f...|\n",
            "|[ or will he just...|[ waste amounts m...|\n",
            "|[ this is fine.\"<...|[ fine.\"<br /><br...|\n",
            "|[ there was \"Raw,...|[ \"Raw,\" really s...|\n",
            "|[ <br /><br />\"Su...|[ <br /><br />\"Su...|\n",
            "|[although it was ...|[although actuall...|\n",
            "|                  []|                  []|\n",
            "|[.at least it wou...|[.at least would ...|\n",
            "|[and the action s...|[and action shot ...|\n",
            "|[but then again, ...|[but again, they'...|\n",
            "|[he meant well. A...|[he meant well. A...|\n",
            "|[but we watched i...|[but watched it, ...|\n",
            "|[ just another un...|[ another unsolve...|\n",
            "|[\". After a coupl...|[\". After couple ...|\n",
            "|[but judge that f...|[but judge yourse...|\n",
            "|[ but actually it...|[ actually kind a...|\n",
            "|          [ kind of]|          [ kind of]|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGfYbg5Hs3eO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0405f8f-2e04-43a8-e24e-3af1afbe0034"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Download NLTK stopwords (only once)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define your UDFs here (replace this with your actual UDF definitions)\n",
        "def remove_stops(text):\n",
        "    # Define your logic to remove stopwords\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stopwords_list]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "remove_stops_udf = udf(remove_stops, StringType())\n",
        "\n",
        "# Define the columns to select from the DataFrame\n",
        "pos_raw_cols = [\"value\"]\n",
        "\n",
        "# Apply the UDF to remove stopwords\n",
        "pos_rm_stops_df = pos_data_df.select(pos_raw_cols).withColumn(\"pos_stop_text\", remove_stops_udf(pos_data_df[\"value\"]))\n",
        "pos_rm_stops_df.show(20)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|               value|       pos_stop_text|\n",
            "+--------------------+--------------------+\n",
            "|[I can only say t...|[I say this: ee03...|\n",
            "|[.maybe.Though I ...|[.maybe.Though I ...|\n",
            "|[..well it doesn'...|[..well really on...|\n",
            "|[and I haven't ev...|[and I even got f...|\n",
            "|[ or will he just...|[ waste amounts m...|\n",
            "|[ this is fine.\"<...|[ fine.\"<br /><br...|\n",
            "|[ there was \"Raw,...|[ \"Raw,\" really s...|\n",
            "|[ <br /><br />\"Su...|[ <br /><br />\"Su...|\n",
            "|[although it was ...|[although actuall...|\n",
            "|                  []|                  []|\n",
            "|[.at least it wou...|[.at least would ...|\n",
            "|[and the action s...|[and action shot ...|\n",
            "|[but then again, ...|[but again, they'...|\n",
            "|[he meant well. A...|[he meant well. A...|\n",
            "|[but we watched i...|[but watched it, ...|\n",
            "|[ just another un...|[ another unsolve...|\n",
            "|[\". After a coupl...|[\". After couple ...|\n",
            "|[but judge that f...|[but judge yourse...|\n",
            "|[ but actually it...|[ actually kind a...|\n",
            "|          [ kind of]|          [ kind of]|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJellzSBIu4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e1838f-e6d5-4e41-80e6-abfd40cd60a8"
      },
      "source": [
        "# Remove features from values\n",
        "import re\n",
        "import string\n",
        "neg_rm_features_df = neg_rm_stops_df.select(neg_raw_cols+[\"neg_stop_text\"]).withColumn(\"neg_feat_text\", remove_features_udf(neg_rm_stops_df[\"neg_stop_text\"]))\n",
        "neg_rm_features_df.show(20)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|               value|       neg_stop_text|       neg_feat_text|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|[I can only say t...|[I say this: ee03...|  say this  portu...|\n",
            "|[.maybe.Though I ...|[.maybe.Though I ...|maybe though  sor...|\n",
            "|[..well it doesn'...|[..well really on...|well really one t...|\n",
            "|[and I haven't ev...|[and I even got f...|and  even got far...|\n",
            "|[ or will he just...|[ waste amounts m...|waste amounts mon...|\n",
            "|[ this is fine.\"<...|[ fine.\"<br /><br...|fine   probably m...|\n",
            "|[ there was \"Raw,...|[ \"Raw,\" really s...|raw really stand ...|\n",
            "|[ <br /><br />\"Su...|[ <br /><br />\"Su...|   surprisingly m...|\n",
            "|[although it was ...|[although actuall...|although actually...|\n",
            "|                  []|                  []|                    |\n",
            "|[.at least it wou...|[.at least would ...|  least would lik...|\n",
            "|[and the action s...|[and action shot ...|and action shot  ...|\n",
            "|[but then again, ...|[but again, they'...|but again they  g...|\n",
            "|[he meant well. A...|[he meant well. A...|  meant well and ...|\n",
            "|[but we watched i...|[but watched it, ...|but watched   did...|\n",
            "|[ just another un...|[ another unsolve...|another unsolved ...|\n",
            "|[\". After a coupl...|[\". After couple ...|after couple case...|\n",
            "|[but judge that f...|[but judge yourse...|but judge yoursel...|\n",
            "|[ but actually it...|[ actually kind a...|actually kind amu...|\n",
            "|          [ kind of]|          [ kind of]|               kind |\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0eelFozuLxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d86560b-9c4f-44bb-8fa1-1957e511dd92"
      },
      "source": [
        "# Remove features from values\n",
        "import re\n",
        "import string\n",
        "pos_rm_features_df = pos_rm_stops_df.select(pos_raw_cols+[\"pos_stop_text\"]).withColumn(\"pos_feat_text\", remove_features_udf(pos_rm_stops_df[\"pos_stop_text\"]))\n",
        "pos_rm_features_df.show(20)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|               value|       pos_stop_text|       pos_feat_text|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|[I can only say t...|[I say this: ee03...|  say this  portu...|\n",
            "|[.maybe.Though I ...|[.maybe.Though I ...|maybe though  sor...|\n",
            "|[..well it doesn'...|[..well really on...|well really one t...|\n",
            "|[and I haven't ev...|[and I even got f...|and  even got far...|\n",
            "|[ or will he just...|[ waste amounts m...|waste amounts mon...|\n",
            "|[ this is fine.\"<...|[ fine.\"<br /><br...|fine   probably m...|\n",
            "|[ there was \"Raw,...|[ \"Raw,\" really s...|raw really stand ...|\n",
            "|[ <br /><br />\"Su...|[ <br /><br />\"Su...|   surprisingly m...|\n",
            "|[although it was ...|[although actuall...|although actually...|\n",
            "|                  []|                  []|                    |\n",
            "|[.at least it wou...|[.at least would ...|  least would lik...|\n",
            "|[and the action s...|[and action shot ...|and action shot  ...|\n",
            "|[but then again, ...|[but again, they'...|but again they  g...|\n",
            "|[he meant well. A...|[he meant well. A...|  meant well and ...|\n",
            "|[but we watched i...|[but watched it, ...|but watched   did...|\n",
            "|[ just another un...|[ another unsolve...|another unsolved ...|\n",
            "|[\". After a coupl...|[\". After couple ...|after couple case...|\n",
            "|[but judge that f...|[but judge yourse...|but judge yoursel...|\n",
            "|[ but actually it...|[ actually kind a...|actually kind amu...|\n",
            "|          [ kind of]|          [ kind of]|               kind |\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3JDtFLfuutb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902fe60c-8989-4620-dea3-aa19ad1202c4"
      },
      "source": [
        "# Tagging text from value\n",
        "import nltk\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "neg_tagged_df = neg_rm_features_df.select(neg_raw_cols+[\"neg_feat_text\"]).withColumn(\"neg_tagged_text\", tag_and_remove_udf(neg_rm_features_df.neg_feat_text))\n",
        "neg_tagged_df.show(20)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|               value|       neg_feat_text|     neg_tagged_text|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|[I can only say t...|  say this  portu...| say portugal say...|\n",
            "|[.maybe.Though I ...|maybe though  sor...| sort enjoyed sat...|\n",
            "|[..well it doesn'...|well really one t...| judge house ramm...|\n",
            "|[and I haven't ev...|and  even got far...| got infant terri...|\n",
            "|[ or will he just...|waste amounts mon...| waste amounts mo...|\n",
            "|[ this is fine.\"<...|fine   probably m...| fine credited op...|\n",
            "|[ there was \"Raw,...|raw really stand ...| raw stand materi...|\n",
            "|[ <br /><br />\"Su...|   surprisingly m...| murphy directed ...|\n",
            "|[although it was ...|although actually...| wallach italian ...|\n",
            "|                  []|                    |                    |\n",
            "|[.at least it wou...|  least would lik...| least like think...|\n",
            "|[and the action s...|and action shot  ...| action shot show...|\n",
            "|[but then again, ...|but again they  g...| good guys acting...|\n",
            "|[he meant well. A...|  meant well and ...| meant well cours...|\n",
            "|[but we watched i...|but watched   did...| watched did coun...|\n",
            "|[ just another un...|another unsolved ...| unsolved mystery...|\n",
            "|[\". After a coupl...|after couple case...| couple cases who...|\n",
            "|[but judge that f...|but judge yoursel...| judge end movie ...|\n",
            "|[ but actually it...|actually kind amu...|       kind amusing |\n",
            "|          [ kind of]|               kind |               kind |\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeMEY_zBGFin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a4a9f7-dcd5-4b9d-d16d-d602e2410989"
      },
      "source": [
        "# Tagging text from value\n",
        "import nltk\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_tagged_df = pos_rm_features_df.select(pos_raw_cols+[\"pos_feat_text\"]).withColumn(\"pos_tagged_text\", tag_and_remove_udf(pos_rm_features_df.pos_feat_text))\n",
        "pos_tagged_df.show(20)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|               value|       pos_feat_text|     pos_tagged_text|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|[I can only say t...|  say this  portu...| say portugal say...|\n",
            "|[.maybe.Though I ...|maybe though  sor...| sort enjoyed sat...|\n",
            "|[..well it doesn'...|well really one t...| judge house ramm...|\n",
            "|[and I haven't ev...|and  even got far...| got infant terri...|\n",
            "|[ or will he just...|waste amounts mon...| waste amounts mo...|\n",
            "|[ this is fine.\"<...|fine   probably m...| fine credited op...|\n",
            "|[ there was \"Raw,...|raw really stand ...| raw stand materi...|\n",
            "|[ <br /><br />\"Su...|   surprisingly m...| murphy directed ...|\n",
            "|[although it was ...|although actually...| wallach italian ...|\n",
            "|                  []|                    |                    |\n",
            "|[.at least it wou...|  least would lik...| least like think...|\n",
            "|[and the action s...|and action shot  ...| action shot show...|\n",
            "|[but then again, ...|but again they  g...| good guys acting...|\n",
            "|[he meant well. A...|  meant well and ...| meant well cours...|\n",
            "|[but we watched i...|but watched   did...| watched did coun...|\n",
            "|[ just another un...|another unsolved ...| unsolved mystery...|\n",
            "|[\". After a coupl...|after couple case...| couple cases who...|\n",
            "|[but judge that f...|but judge yoursel...| judge end movie ...|\n",
            "|[ but actually it...|actually kind amu...|       kind amusing |\n",
            "|          [ kind of]|               kind |               kind |\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX9RYWxIQKM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d74f01-6375-4306-cf2f-cc4fa6fa8e6b"
      },
      "source": [
        "# Lemmatization for values\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "neg_lemm_df = neg_tagged_df.select(neg_raw_cols+[\"neg_tagged_text\"]).withColumn(\"neg_lemm_text\", lemmatize_udf(neg_tagged_df[\"neg_tagged_text\"]))\n",
        "neg_lemm_df.show(20)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|               value|     neg_tagged_text|       neg_lemm_text|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|[I can only say t...| say portugal say...|say portugal say ...|\n",
            "|[.maybe.Though I ...| sort enjoyed sat...|sort enjoy satani...|\n",
            "|[..well it doesn'...| judge house ramm...|judge house ram b...|\n",
            "|[and I haven't ev...| got infant terri...|get infant terrib...|\n",
            "|[ or will he just...| waste amounts mo...|waste amount mone...|\n",
            "|[ this is fine.\"<...| fine credited op...|fine credit open ...|\n",
            "|[ there was \"Raw,...| raw stand materi...|raw stand materia...|\n",
            "|[ <br /><br />\"Su...| murphy directed ...|murphy direct mov...|\n",
            "|[although it was ...| wallach italian ...|wallach italian v...|\n",
            "|                  []|                    |                    |\n",
            "|[.at least it wou...| least like think...|least like think ...|\n",
            "|[and the action s...| action shot show...|action shot show ...|\n",
            "|[but then again, ...| good guys acting...|good guy act impr...|\n",
            "|[he meant well. A...| meant well cours...|meant well course...|\n",
            "|[but we watched i...| watched did coun...|watch do count ma...|\n",
            "|[ just another un...| unsolved mystery...|unsolved mystery ...|\n",
            "|[\". After a coupl...| couple cases who...|couple case whole...|\n",
            "|[but judge that f...| judge end movie ...|judge end movie l...|\n",
            "|[ but actually it...|       kind amusing |        kind amusing|\n",
            "|          [ kind of]|               kind |                kind|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmkjqfcRvgzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a953aef2-32cc-41bc-b7ab-3002eb8fc9b2"
      },
      "source": [
        "# Lemmatization for values\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "pos_lemm_df = pos_tagged_df.select(pos_raw_cols+[\"pos_tagged_text\"]).withColumn(\"pos_lemm_text\", lemmatize_udf(pos_tagged_df[\"pos_tagged_text\"]))\n",
        "pos_lemm_df.show(20)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|               value|     pos_tagged_text|       pos_lemm_text|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|[I can only say t...| say portugal say...|say portugal say ...|\n",
            "|[.maybe.Though I ...| sort enjoyed sat...|sort enjoy satani...|\n",
            "|[..well it doesn'...| judge house ramm...|judge house ram b...|\n",
            "|[and I haven't ev...| got infant terri...|get infant terrib...|\n",
            "|[ or will he just...| waste amounts mo...|waste amount mone...|\n",
            "|[ this is fine.\"<...| fine credited op...|fine credit open ...|\n",
            "|[ there was \"Raw,...| raw stand materi...|raw stand materia...|\n",
            "|[ <br /><br />\"Su...| murphy directed ...|murphy direct mov...|\n",
            "|[although it was ...| wallach italian ...|wallach italian v...|\n",
            "|                  []|                    |                    |\n",
            "|[.at least it wou...| least like think...|least like think ...|\n",
            "|[and the action s...| action shot show...|action shot show ...|\n",
            "|[but then again, ...| good guys acting...|good guy act impr...|\n",
            "|[he meant well. A...| meant well cours...|meant well course...|\n",
            "|[but we watched i...| watched did coun...|watch do count ma...|\n",
            "|[ just another un...| unsolved mystery...|unsolved mystery ...|\n",
            "|[\". After a coupl...| couple cases who...|couple case whole...|\n",
            "|[but judge that f...| judge end movie ...|judge end movie l...|\n",
            "|[ but actually it...|       kind amusing |        kind amusing|\n",
            "|          [ kind of]|               kind |                kind|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqjLxLRcRkGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348ba07c-87fb-4e5e-f994-b28e081dfaa7"
      },
      "source": [
        "neg_check_blanks_df = neg_lemm_df.select(neg_raw_cols+[\"neg_lemm_text\"]).withColumn(\"is_blank\", check_blanks_udf(neg_lemm_df[\"neg_lemm_text\"]))\n",
        "# remove blanks from lemmatized text\n",
        "neg_no_blanks_df = neg_check_blanks_df.filter(neg_check_blanks_df[\"is_blank\"] == \"False\")\n",
        "\n",
        "# drop duplicates from lemmatized text\n",
        "neg_dedup_df = neg_no_blanks_df.dropDuplicates(['neg_lemm_text'])\n",
        "\n",
        "neg_dedup_df.show(20)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------+\n",
            "|               value|       neg_lemm_text|is_blank|\n",
            "+--------------------+--------------------+--------+\n",
            "|[ From then on, t...|action swift loca...|   False|\n",
            "|[ Amitabh is 65 a...|        amitabh tabu|   False|\n",
            "|[ for artistic pu...|artistic purpose ...|   False|\n",
            "|[). I assume most...|assume actor gain...|   False|\n",
            "|[ and all her bab...|babble go change ...|   False|\n",
            "|[was there a scri...|be script villain...|   False|\n",
            "|  [ become one with]|              become|   False|\n",
            "|[ finally, but a ...|bit late get litt...|   False|\n",
            "|[uh, bite film th...|bite film briefly...|   False|\n",
            "|[ Boll.Lorenzo La...|boll lorenzo lama...|   False|\n",
            "|[OK, the box look...|box look promise ...|   False|\n",
            "|[. bulls**t! This...|bull film try any...|   False|\n",
            "|[ in comparison w...|comparison compli...|   False|\n",
            "|[..<br /><br />We...|crap act include ...|   False|\n",
            "|[ well, all that ...|crap interest fre...|   False|\n",
            "|[ but a completel...|different subject...|   False|\n",
            "|[discriminating. ...|discriminate most...|   False|\n",
            "|[<br /><br />Dist...|distributor movie...|   False|\n",
            "|[ which is what t...|documentary do ho...|   False|\n",
            "|[ it doesn't, ins...|doesn cliff hanga...|   False|\n",
            "+--------------------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMzHtJX_yLms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4684a140-e6e6-4edd-faab-3f8e5553a866"
      },
      "source": [
        "pos_check_blanks_df = pos_lemm_df.select(pos_raw_cols+[\"pos_lemm_text\"]).withColumn(\"is_blank\", check_blanks_udf(pos_lemm_df[\"pos_lemm_text\"]))\n",
        "# remove blanks from lemmatized text\n",
        "pos_no_blanks_df = pos_check_blanks_df.filter(pos_check_blanks_df[\"is_blank\"] == \"False\")\n",
        "\n",
        "# drop duplicates from lemmatized text\n",
        "pos_dedup_df = pos_no_blanks_df.dropDuplicates(['pos_lemm_text'])\n",
        "\n",
        "pos_dedup_df.show(20)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------+\n",
            "|               value|       pos_lemm_text|is_blank|\n",
            "+--------------------+--------------------+--------+\n",
            "|[ From then on, t...|action swift loca...|   False|\n",
            "|[ Amitabh is 65 a...|        amitabh tabu|   False|\n",
            "|[ for artistic pu...|artistic purpose ...|   False|\n",
            "|[). I assume most...|assume actor gain...|   False|\n",
            "|[ and all her bab...|babble go change ...|   False|\n",
            "|[was there a scri...|be script villain...|   False|\n",
            "|  [ become one with]|              become|   False|\n",
            "|[ finally, but a ...|bit late get litt...|   False|\n",
            "|[uh, bite film th...|bite film briefly...|   False|\n",
            "|[ Boll.Lorenzo La...|boll lorenzo lama...|   False|\n",
            "|[OK, the box look...|box look promise ...|   False|\n",
            "|[. bulls**t! This...|bull film try any...|   False|\n",
            "|[ in comparison w...|comparison compli...|   False|\n",
            "|[..<br /><br />We...|crap act include ...|   False|\n",
            "|[ well, all that ...|crap interest fre...|   False|\n",
            "|[ but a completel...|different subject...|   False|\n",
            "|[discriminating. ...|discriminate most...|   False|\n",
            "|[<br /><br />Dist...|distributor movie...|   False|\n",
            "|[ which is what t...|documentary do ho...|   False|\n",
            "|[ it doesn't, ins...|doesn cliff hanga...|   False|\n",
            "+--------------------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmkd0xHt7TdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acc6a680-c801-4ee7-8f6e-0807b600aa1d"
      },
      "source": [
        "# Rename final column as text for processing\n",
        "pos_final_df = pos_dedup_df.selectExpr(\"pos_lemm_text as text\")\n",
        "pos_final_df.show()\n",
        "pos_final_df.printSchema()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|open smell urine ...|\n",
            "|star frothy roman...|\n",
            "|rest actor poor s...|\n",
            "|babble go change ...|\n",
            "|first time hop ma...|\n",
            "|undemanding way s...|\n",
            "|least like think ...|\n",
            "|different subject...|\n",
            "|be script villain...|\n",
            "|matter element ma...|\n",
            "|wet bandit wait l...|\n",
            "|think see worst m...|\n",
            "|crap act include ...|\n",
            "|give dunno effort...|\n",
            "|don think look an...|\n",
            "|film make documen...|\n",
            "|doesn cliff hanga...|\n",
            "|bite film briefly...|\n",
            "|uninteresting peo...|\n",
            "|judge house ram b...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- text: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ik1p98Ey-5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ec7b66-8546-4279-e99a-846c729b70ea"
      },
      "source": [
        "# Rename final column as text for processing\n",
        "neg_final_df = neg_dedup_df.selectExpr(\"neg_lemm_text as text\")\n",
        "neg_final_df.show()\n",
        "neg_final_df.printSchema()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|open smell urine ...|\n",
            "|star frothy roman...|\n",
            "|rest actor poor s...|\n",
            "|babble go change ...|\n",
            "|first time hop ma...|\n",
            "|undemanding way s...|\n",
            "|least like think ...|\n",
            "|different subject...|\n",
            "|be script villain...|\n",
            "|matter element ma...|\n",
            "|wet bandit wait l...|\n",
            "|think see worst m...|\n",
            "|crap act include ...|\n",
            "|give dunno effort...|\n",
            "|don think look an...|\n",
            "|film make documen...|\n",
            "|doesn cliff hanga...|\n",
            "|bite film briefly...|\n",
            "|uninteresting peo...|\n",
            "|judge house ram b...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- text: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM7E-uXZSHs3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efaf306a-e706-4505-ddc8-b5e08c6ca75c"
      },
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "# Create Unique ID for text\n",
        "neg_uid_df = neg_final_df.withColumn(\"neg_uid\", monotonically_increasing_id())\n",
        "neg_uid_df.show(4)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+\n",
            "|                text|neg_uid|\n",
            "+--------------------+-------+\n",
            "|open smell urine ...|      0|\n",
            "|star frothy roman...|      1|\n",
            "|rest actor poor s...|      2|\n",
            "|babble go change ...|      3|\n",
            "+--------------------+-------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wRnoUdXzZVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a830d0bb-63f0-49ff-b512-7f19c59cab95"
      },
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "# Create Unique ID for text\n",
        "pos_uid_df = pos_final_df.withColumn(\"pos_uid\", monotonically_increasing_id())\n",
        "pos_uid_df.show(4)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+\n",
            "|                text|pos_uid|\n",
            "+--------------------+-------+\n",
            "|open smell urine ...|      0|\n",
            "|star frothy roman...|      1|\n",
            "|rest actor poor s...|      2|\n",
            "|babble go change ...|      3|\n",
            "+--------------------+-------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbEqaJOg87He",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0890f809-3e0c-4dba-e8b9-00502d487b42"
      },
      "source": [
        "# Create label with 1.0 being the constant for ML classifier\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "@udf(\"double\")\n",
        "def const_col():\n",
        "    return 1.0\n",
        "\n",
        "neg_label_df = neg_uid_df.withColumn('label', const_col())\n",
        "neg_label_df.show()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+-----+\n",
            "|                text|neg_uid|label|\n",
            "+--------------------+-------+-----+\n",
            "|open smell urine ...|      0|  1.0|\n",
            "|star frothy roman...|      1|  1.0|\n",
            "|rest actor poor s...|      2|  1.0|\n",
            "|babble go change ...|      3|  1.0|\n",
            "|first time hop ma...|      4|  1.0|\n",
            "|undemanding way s...|      5|  1.0|\n",
            "|least like think ...|      6|  1.0|\n",
            "|different subject...|      7|  1.0|\n",
            "|be script villain...|      8|  1.0|\n",
            "|matter element ma...|      9|  1.0|\n",
            "|wet bandit wait l...|     10|  1.0|\n",
            "|think see worst m...|     11|  1.0|\n",
            "|crap act include ...|     12|  1.0|\n",
            "|give dunno effort...|     13|  1.0|\n",
            "|don think look an...|     14|  1.0|\n",
            "|film make documen...|     15|  1.0|\n",
            "|doesn cliff hanga...|     16|  1.0|\n",
            "|bite film briefly...|     17|  1.0|\n",
            "|uninteresting peo...|     18|  1.0|\n",
            "|judge house ram b...|     19|  1.0|\n",
            "+--------------------+-------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTmOwp420GhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f693b975-669d-4349-9746-d43ccda4c0d6"
      },
      "source": [
        "# Create label with 1.0 being the constant for ML classifier\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "@udf(\"double\")\n",
        "def const_col():\n",
        "    return 1.0\n",
        "\n",
        "pos_label_df = pos_uid_df.withColumn('label', const_col())\n",
        "pos_label_df.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+-----+\n",
            "|                text|pos_uid|label|\n",
            "+--------------------+-------+-----+\n",
            "|open smell urine ...|      0|  1.0|\n",
            "|star frothy roman...|      1|  1.0|\n",
            "|rest actor poor s...|      2|  1.0|\n",
            "|babble go change ...|      3|  1.0|\n",
            "|first time hop ma...|      4|  1.0|\n",
            "|undemanding way s...|      5|  1.0|\n",
            "|least like think ...|      6|  1.0|\n",
            "|different subject...|      7|  1.0|\n",
            "|be script villain...|      8|  1.0|\n",
            "|matter element ma...|      9|  1.0|\n",
            "|wet bandit wait l...|     10|  1.0|\n",
            "|think see worst m...|     11|  1.0|\n",
            "|crap act include ...|     12|  1.0|\n",
            "|give dunno effort...|     13|  1.0|\n",
            "|don think look an...|     14|  1.0|\n",
            "|film make documen...|     15|  1.0|\n",
            "|doesn cliff hanga...|     16|  1.0|\n",
            "|bite film briefly...|     17|  1.0|\n",
            "|uninteresting peo...|     18|  1.0|\n",
            "|judge house ram b...|     19|  1.0|\n",
            "+--------------------+-------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_FVwbzLSY7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63adf333-3328-4511-86a5-51243635ee8a"
      },
      "source": [
        "pos_data = pos_label_df.select('pos_uid','text','label')\n",
        "pos_data.show(20)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+-----+\n",
            "|pos_uid|                text|label|\n",
            "+-------+--------------------+-----+\n",
            "|      0|open smell urine ...|  1.0|\n",
            "|      1|star frothy roman...|  1.0|\n",
            "|      2|rest actor poor s...|  1.0|\n",
            "|      3|babble go change ...|  1.0|\n",
            "|      4|first time hop ma...|  1.0|\n",
            "|      5|undemanding way s...|  1.0|\n",
            "|      6|least like think ...|  1.0|\n",
            "|      7|different subject...|  1.0|\n",
            "|      8|be script villain...|  1.0|\n",
            "|      9|matter element ma...|  1.0|\n",
            "|     10|wet bandit wait l...|  1.0|\n",
            "|     11|think see worst m...|  1.0|\n",
            "|     12|crap act include ...|  1.0|\n",
            "|     13|give dunno effort...|  1.0|\n",
            "|     14|don think look an...|  1.0|\n",
            "|     15|film make documen...|  1.0|\n",
            "|     16|doesn cliff hanga...|  1.0|\n",
            "|     17|bite film briefly...|  1.0|\n",
            "|     18|uninteresting peo...|  1.0|\n",
            "|     19|judge house ram b...|  1.0|\n",
            "+-------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI_fZ2SG0bMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7b2537-2ce6-4a2b-9a0d-b7d47bab52f6"
      },
      "source": [
        "neg_data = neg_label_df.select('neg_uid','text','label')\n",
        "neg_data.show(20)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+-----+\n",
            "|neg_uid|                text|label|\n",
            "+-------+--------------------+-----+\n",
            "|      0|open smell urine ...|  1.0|\n",
            "|      1|star frothy roman...|  1.0|\n",
            "|      2|rest actor poor s...|  1.0|\n",
            "|      3|babble go change ...|  1.0|\n",
            "|      4|first time hop ma...|  1.0|\n",
            "|      5|undemanding way s...|  1.0|\n",
            "|      6|least like think ...|  1.0|\n",
            "|      7|different subject...|  1.0|\n",
            "|      8|be script villain...|  1.0|\n",
            "|      9|matter element ma...|  1.0|\n",
            "|     10|wet bandit wait l...|  1.0|\n",
            "|     11|think see worst m...|  1.0|\n",
            "|     12|crap act include ...|  1.0|\n",
            "|     13|give dunno effort...|  1.0|\n",
            "|     14|don think look an...|  1.0|\n",
            "|     15|film make documen...|  1.0|\n",
            "|     16|doesn cliff hanga...|  1.0|\n",
            "|     17|bite film briefly...|  1.0|\n",
            "|     18|uninteresting peo...|  1.0|\n",
            "|     19|judge house ram b...|  1.0|\n",
            "+-------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y25NYlxGSiU3"
      },
      "source": [
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(pos_trainingData, pos_testData) = pos_data.randomSplit([0.7, 0.3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J70_3H9o03y_"
      },
      "source": [
        "# Split the data into training and test sets (40% held out for testing)\n",
        "(neg_trainingData, neg_testData) = neg_data.randomSplit([0.7, 0.3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running Naive Bayes classifier.\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, DecisionTreeClassifier\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "from pyspark.ml.tuning import CrossValidator\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "#data = tokenizer.transform(data)\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
        "labelEncoder = StringIndexer(inputCol=\"text\", outputCol='label')\n",
        "#vectorizer = CountVectorizer(inputCol= \"words\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(minDocFreq=2000, inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "#assembler = VectorAssembler(inputCols=[\"hour\", \"mobile\", \"userFeatures\"],outputCol=\"features\")\n",
        "#idfModel = idf.fit(data)\n",
        "\n",
        "#lda = LDA(k=20, seed=1, optimizer=\"em\")\n",
        "\n",
        "# DecisionTree Classifier\n",
        "#dt = DecisionTreeClassifier()\n",
        "\n",
        "# Linear Support Vector Classifer\n",
        "#lsvc = LinearSVC((maxIter=10, regParam=0.1)\n",
        "\n",
        "# Naive Bayes model\n",
        "nb = NaiveBayes(smoothing=2.0)\n",
        "\n",
        "# Random Forest Classifier\n",
        "#rfc = RandomForestClassifier()\n",
        "\n",
        "# Pipeline Architecture for NB\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
        "\n",
        "# Pipeline Architecture for RFC\n",
        "#pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, rfc])\n",
        "\n",
        "# Pipeline Architecture for DT\n",
        "#pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, dt])\n",
        "\n",
        "# Pipeline Architecture for LSVC\n",
        "#pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lsvc])"
      ],
      "metadata": {
        "id": "StMRYnDVKJL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model.  This also runs the indexers.\n",
        "neg_model = pipeline.fit(neg_trainingData)\n",
        "neg_model\n"
      ],
      "metadata": {
        "id": "vUzmSwJAeyFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model.  This also runs the indexers.\n",
        "pos_model = pipeline.fit(pos_trainingData)\n",
        "pos_model\n"
      ],
      "metadata": {
        "id": "wm3I9zy2e8bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2DlsYuZfPQp"
      },
      "source": [
        "pos_predictions = pos_model.transform(pos_testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "pos_predictions.select(\"text\", \"label\", \"prediction\").show(5,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4h7JIBih-LG"
      },
      "source": [
        "neg_predictions = neg_model.transform(neg_testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "neg_predictions.select(\"text\", \"label\", \"prediction\").show(5,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTKAQRoRsZ3N"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "neg_evaluator = RegressionEvaluator(predictionCol=\"prediction\")\n",
        "neg_evaluator.evaluate(neg_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkmjxK6pkqTi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3f416a-2153-4dba-8e65-5eb24e6fdcee"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "pos_evaluator = RegressionEvaluator(predictionCol=\"prediction\")\n",
        "pos_evaluator.evaluate(pos_predictions)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9pvzLFwsO0X"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "pos_evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "pos_evaluator.evaluate(pos_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlccfDu8k6fm"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "neg_evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "neg_evaluator.evaluate(neg_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0yk6qGGoLj_"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "pos_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
        "pos_evaluator.evaluate(pos_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29UeX35QlgHE"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "neg_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
        "neg_evaluator.evaluate(neg_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVSiOLUaOobM"
      },
      "source": [
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "def sentiment_analysis(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "sentiment_analysis_udf = udf(sentiment_analysis , FloatType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PErj_fJ2OzZV"
      },
      "source": [
        "df = neg_label_df.withColumn(\"sentiment_score\", sentiment_analysis_udf(neg_label_df['text'] ))\n",
        "df.show(20,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBsJvrfsnzWj"
      },
      "source": [
        "df2 = pos_label_df.withColumn(\"sentiment_score\", sentiment_analysis_udf(pos_label_df['text'] ))\n",
        "df2.show(20,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LXdpO1dO45e"
      },
      "source": [
        "def condition(r):\n",
        "    if (r >=0.1):\n",
        "        label = \"positive\"\n",
        "    elif(r <= -0.1):\n",
        "        label = \"negative\"\n",
        "    else:\n",
        "        label = \"neutral\"\n",
        "    return label\n",
        "\n",
        "sentiment_udf = udf(lambda x: condition(x), StringType())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}